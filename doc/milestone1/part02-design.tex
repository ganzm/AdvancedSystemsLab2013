\documentclass[milestone1.tex]{subfiles} 
\begin{document}

\section{Design Decisions}

In this section a selection of the more important design decisions is presented. It is described why these choices were made and how they affect the behaviour of the system.

\subsection{Blocking vs. Nonblocking Network IO}
Java basically allows two ways how to handle tcp/ip connections. There is the more straight forward way with blocking io and there is another API which allows nonblocking operation on sockets.

The implementation of the middle ware features a single threaded networking software layer which implements the reactor pattern by using a \textit{java.nio.channels.Selector}. The basic idea is to let the Selector keep track of connections. By a connection based state variable we let the Selector know in which events we are interested in.

\paragraph{Advantages}
By using nonblocking io over the blocking method, we avoid that the number of threads in the system need to be scaled with the number of clients. It is assumed that system scales better in terms of concurrent connections than the blocking method for the simple fact that thread switches takes time and memory.

\subsection{Database Connection Pooling}
Establishing a jdbc database connection takes time. A Connection Pool keeps a list of database connections which are permanently opened. As soon as a connection is needed by a worker, it simply queries the pool for one.

The pool needs to be accessed in an exclusive fashion. However the cost to avoid concurrent pool access should be way lower than creating a database connection each time one is needed.

A nice effect of having a permanent database connection is that it allows the use of PreparedStatement. These are sql statements which can be precompiled on the database system.

\subsection{Mutual Exclusion on the Database}

A problem which arose later in the project was a bug where there were two messages in a queue and two clients tried to dequeue messages from it simultaneous. Dequeuing a message is performed in two separate sql statements. The first statement peeks a message and the second one tries to delete the peeked one. The bug occured where two client peeked the same message but only one of them could delete it. A simple way of resolving that issue would have been to put both statements described into a single prepared statement and additionally set the database isolation level to \textit{serialized}. 

We decided to fix this bug by locking critical records. If a client tries to dequeue a message it selects and locks it with a "Select ... For Update' sql statement. The selected record is now exclusively locked for the current transaction. Other clients which try to get a lock on the very same record end up being blocked by the database system until the first client commits its transaction.

\paragraph{Another option}
Having a connection pool could have been avoided if every client was assigned a dedicated database connection. There would have been no need to synchronize worker threads to retrieve a connection. This however decreases the number of tuning parameter. Database connection would not have been optimally used. Connections would have been idling for each time consuming operation a worker performs.

\subsection{Buffer Pooling}
\label{sub:buffer-pooling}
Serializing and deserializing messages which are up to 2kb long requires a buffer of at least the same size. In order to avoid excessive garbage collection it was decided to implement a pool containing message buffers.

Pooled buffers are obtained from the networking interface before receiving a message. After processing the message and right before we lose the reference to the buffer it is put back to the pool for later use.

Having this pool should significantly increase the period between garbage collection.

\paragraph{Note}
See implementation of \textit{ch.ethz.mlmq.nio.ByteBufferPool}

\subsection{Message Transmission}
Messages are passed on the initiative of a client (There is no way for the messaging system to push information to a specific client). A clients issues a requests and expects a response within a certain time. Each requests generates a response. In case of an error an error response is replied by the middle ware.

A message consists of a 4 byte length field and a variable length field containing the payload.

To send a messages (requests and responses) the first 4 bytes indicate the binary length of the message. After that the request (or response) is serialized.

\subsection{Message Serialisation}

The first approach to serialize messages was using the java's built in stream based serialisation infrastructure. This however did not work well together with \ref{sub:buffer-pooling}. Default java serialisation won't let us plug in our buffer pool. Further the decision to implement the networking interface with java nonblocking io however forced us to use java's \textit{ByteBuffer} class to handle binary data.

Theses issues brought us to the decision to implement custom serialisation and deserialisation supporting pooled ByteBuffers.

\subsection{Load Shedding}
With the current design requests are queued in a single request queue. If workers are slow requests begin to queue up. If the queue is full adding another request fails. The request from the client is silently dropped. This behaviour is commonly known as load shedding. The system tries as quick as possible to get rid of load which cant be handled any more. The point where the system starts trashing will be moved further away.



\end{document}