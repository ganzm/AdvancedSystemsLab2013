\documentclass[milestone1.tex]{subfiles} 
\begin{document}

\section{Design Decisions}

In this section a selection of the more important design decisions is presented. It is described why these choices were made and how they affect the behaviour of the system.

\subsection{Blocking vs. Nonblocking Network IO}
Java basically allows two ways how to handle tcp/ip connections. There is the more straight forward way with blocking io and there is another api which allows nonblocking operation on sockets.

The implementation of the middle ware features a single threaded networking software layer which implements the reactor pattern by using a \textit{java.nio.channels.Selector}. The basic idea is to let the Selector keep track of connections. By a connection based configuration we let the Selector know in which events we are interested in.

\paragraph{Advantages}
By using nonblocking io over the blocking method, we avoid that the number of threads in the system need to be scaled to the number of clients to simultaneously serve. It is assumed that system scales better in terms concurrent connections thant the blocking method for the simple fact that thread switches takes time and memory.

\subsection{Database Connection Pooling}
Establishing a jdbc database connection takes time. A Connection Pool keeps a list of database connections which are permanently opened. As soon as a connection is needed by a worker, it simply queries the pool for one.

The pool needs to be accessed in an exclusive fashion. However the cost to avoid concurrent pool access should be way lower than creating a database connection each time one is needed.

A nice effect of having a permanent database connection is that it allows the use of PreparedStatement. Sql statements which can be precompiled on the database system.

\paragraph{Another option}
Having a connection pool could have been avoided if every client was assigned a dedicated database connection. There would have been no need to synchronize worker threads to retrieve a connection. This however decreases the number of tuning parameter. Database connection would not have been optimally used. Connections would have been idling for each time consuming operation a worker performs.

\subsection{Buffer Pooling}
\label{sub:buffer-pooling}
Serializing and deserializing messages which are up to 2kb long requires a buffer of at least the same size. In order to avoid excessive garbage collection it was decided to implement a pool containing message buffers.

Pooled buffers are obtained from the networking interface before receiving a message. After processing the message and right before we lose the reference to the buffer it is put back to the pool for later use.

Having this pool should significantly increase the period between garbage collection.

\paragraph{Note}
See implementation of \textit{ch.ethz.mlmq.nio.ByteBufferPool}

\subsection{Message Transmission}
Messages are passed on the initiative of a client (There is no way for the messaging system to push information to a specific client). A clients issues a requests and expects a response within a certain time. Each requests generates a response. In case of an error an error response is replied by the middle ware.

A message consists of a 4 byte length field and a variable length field containing the payload.

To send a messages (requests and responses) the first 4 bytes indicate the binary length of the message. After that the request (or response) is serialized.

\subsection{Message Serialisation}

The first approach to serialize messages was using the java's built in stream based serialisation infrastructure. This however did not work well together with \ref{sub:buffer-pooling}. Default java serialisation won't let us plug in our buffer pool. Furhter the decision to implement the networking interface with java nonblocking io however forced us to use java's \textit{ByteBuffer} class to handle binary data.

Theses issues brought us to the decision to implement custom serialisation and deserialisation supporting pooled ByteBuffers.

\subsection{Load Shedding}
With the current design requests are queued in a single request queue. If workers are slow requests begin to queue up. If the queue is full adding another request fails. The request from the client is silently dropped.

\end{document}